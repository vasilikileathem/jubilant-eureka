import pyspark.sql.functions as F
from pyspark.sql import types as T
from pyspark.sql import SparkSession


spark = SparkSession \
    .builder \
    .appName("Json Conversion to Parquet") \
    .getOrCreate()


def flatten(df):
    complex_fields = dict([(field.name, field.dataType) for field in df.schema.fields if isinstance(field.dataType, T.ArrayType) or isinstance(field.dataType, T.StructType)])
    qualify = list(complex_fields.keys())[0] + "_"
    while len(complex_fields) != 0:
        col_name = list(complex_fields.keys())[0]
        if isinstance(complex_fields[col_name], T.StructType):
            expanded = [F.col(col_name + '.' + k).alias(col_name + '_' + k) for k in [n.name for n in complex_fields[col_name]]]
            df = df.select("*", *expanded).drop(col_name)
        elif isinstance(complex_fields[col_name], T.ArrayType):
            df = df.withColumn(col_name, F.explode(col_name))
        complex_fields = dict([(field.name, field.dataType) for field in df.schema.fields if isinstance(field.dataType, T.ArrayType) or isinstance(field.dataType, T.StructType)])
    for df_col_name in df.columns:
        df = df.withColumnRenamed(df_col_name, df_col_name.replace(qualify, ""))
    return df


df = spark.read.option("multiline", "true").json("sample.json")

df_flatten = flatten(df)

df_flatten.show(truncate=False)

df.write.parquet("s3a://bucket-name/path/to/name.parquet", mode="overwrite")
